{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorporating fine grained/additional labels with custom loss function in Google Contrail competition\n",
    "\n",
    "## The competition\n",
    "\n",
    "[Google Research - Identify Contrails to Reduce Global Warming](https://www.kaggle.com/competitions/google-research-identify-contrails-reduce-global-warming/overview) was hosted on Kaggle and tasked competitors to create a ML model which creates a pixel mask marking contrails on sattelite data.\n",
    "The images linked here are taken from googles accompanying material.  \n",
    "\n",
    "![contrails](https://storage.googleapis.com/kaggle-media/competitions/Google-Contrails/waterdroplets.png)\n",
    "\n",
    "![masks](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F59561%2F590a0bc76044a4ceb71368cf3b62412e%2Fcontrails_600.png?generation=1683669162469942&alt=media)\n",
    "\n",
    "### Labels\n",
    "\n",
    "One of the interesting features of the given data was that besides a ground truth mask, additionally the individual masks of each human annotator was given. The ground truth was computed from the individual masks.\n",
    "\n",
    "> Ground truth was determined by (generally) 4+ different labelers annotating each image. Pixels were considered a contrail when >50% of the labelers annotated it as such. Individual annotations (human_individual_masks.npy) as well as the aggregated ground truth annotations (human_pixel_masks.npy) are included in the training data. The validation data only includes the aggregated ground truth annotations.\n",
    "\n",
    "The stark differences between each individual mask as well as between the individual masks and the ground truth computed from the masks can easily be seen in an example. \n",
    "\n",
    "Ground truth\n",
    "![ground truth](./ground_truth.png)\n",
    "Individual masks\n",
    "![Individual masks](./different_masks.png)\n",
    "\n",
    "\n",
    "### Evaluation metric\n",
    "\n",
    "> This competition is evaluated on the global Dice coefficient.\n",
    "\n",
    "## My experiment\n",
    "b2\n",
    "Seeing as there was on the one hand a lot of contrails like objects which were spotted only by single annotators\n",
    "\n",
    "b3\n",
    "Seeing the stark contrast between individual annotators masks and also between those and the thereof computed ground truth, I felt that including the individual annotators masks should be a big information gain for the model.\n",
    "\n",
    "A simple way to include this information and still gain knowledge seemed to me to punish false positives less if they are present in at least one individual mask.\n",
    "\n",
    "Potentially, this could be combined with punishing false negatives of the ground truth harder as well.\n",
    "\n",
    "To implement theses ideas in a custom loss function, I use the F-beta-loss function of pytorch with appropriate beta values for each of the two components, instead of only using the F-1/Dice score on the ground truth mask.  \n",
    "\n",
    "\n",
    "My goal was to find good hyperparameters for a custom loss function which weighs (i) A F-beta-Score of the ground truth mask with (ii) A F-beta-Score of the Union of the individual masks.\n",
    "\n",
    "**My intuition** was that choosing the betas such that (i)slightly penalizing false positives on the ground truth mask and (ii) penalizing false negatives more on the union of all individual masks should lead to a better result than just using a F-1-Score(/Dice Score) on the ground truth as basis for the loss function.\n",
    "\n",
    "### Ground truth vs union of individual masks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show results!!!!\n",
    "Copy some of the output plots which compares the different hyperparameter of the loss function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code\n",
    "The code builds upon Egor Trushins' Notebook [[GR-ICRGW] PL Pipeline Improved](https://www.kaggle.com/code/egortrushin/gr-icrgw-pl-pipeline-improved). \n",
    "\n",
    "## Segmentation model\n",
    "I am using [pytorch lightning](https://lightning.ai/pytorch-lightning) and [segmetation_models_pytorch](https://smp.readthedocs.io/en/latest/models.html#unet).  \n",
    "As segmentation model I use Unet with timm-ResNeSt26d from smp.  \n",
    "## Data\n",
    "The given training and validation data is massive.  \n",
    "Per data-point in the training data set, there are: \n",
    "- Satellite images of eight time steps. Four before and three after timesteps, and the timestep the annotators saw.\n",
    "- Each timestep consists of an satellite image of 9 infrared channels. \n",
    "- Four or more individual pixel masks as well as a ground truth mask\n",
    "Validation data lacks the individual pixel masks, otherwise it's the same.\n",
    "Instead of utilizing all the infrared bands, I use the rgb ash colour scheme computed from bands 11,13,15 which is also used to show the satellite images to the annotators.\n",
    "\n",
    "And instead of trying to use all time steps I only use timestep t0.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# import h5py11\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import CSVLogger, TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping, TQDMProgressBar\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import segmentation_models_pytorch as smp\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "from torch.optim import AdamW\n",
    "from torchmetrics.functional import dice\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "import yaml\n",
    "from transformers import get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "# from pprint import pprint\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "torch.set_float32_matmul_precision('medium')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "The config is used by the dataset and the lightning module to set and load model, trainer and dataset parameters.  \n",
    "The loss function hyperparameters are saved here as well, but changed via the training function parameter later per model.\n",
    "\n",
    "*folds should be deleted here as they are not used in this experiment*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile resnest26d_ash_attention_sample_loss.yaml\n",
    "\n",
    "\n",
    "output_dir: \"./resnest26d_ash_attention_sample_loss\"\n",
    "\n",
    "seed: 42\n",
    "\n",
    "train_bs: 68\n",
    "valid_bs: 128\n",
    "workers: 6\n",
    "\n",
    "progress_bar_refresh_rate: 1\n",
    "\n",
    "early_stop:\n",
    "    monitor: \"val_loss\"\n",
    "    mode: \"min\"\n",
    "    patience: 5\n",
    "    verbose: 1\n",
    "\n",
    "trainer:\n",
    "    max_epochs: 30\n",
    "    min_epochs: 24\n",
    "    enable_progress_bar: True\n",
    "    precision: \"16-mixed\"\n",
    "    devices: 1\n",
    "\n",
    "model:\n",
    "    alpha: -1.0\n",
    "    beta: -1.0\n",
    "    alpha_union: -1.0\n",
    "    beta_union: -1.0\n",
    "    weight_union: -1.0\n",
    "    weight_ground_truth: -1.0\n",
    "    seg_model: \"Unet\"\n",
    "    encoder_name: \"timm-resnest26d\"\n",
    "    encoder_depth: 5\n",
    "    loss_smooth: 1.0\n",
    "    decoder_attention_type: \"scse\"\n",
    "    image_size: 384\n",
    "    optimizer_params:\n",
    "        lr: 0.0005\n",
    "        weight_decay: 0.0\n",
    "        eps: 1.0e-6\n",
    "    scheduler:\n",
    "        name: \"CosineAnnealingLR\"\n",
    "        params:\n",
    "            CosineAnnealingLR:\n",
    "                T_max: 2\n",
    "                eta_min: 1.0e-6\n",
    "                last_epoch: -1\n",
    "            ReduceLROnPlateau:\n",
    "                mode: \"min\"\n",
    "                factor: 0.31622776601\n",
    "                patience: 4\n",
    "                verbose: True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"resnest26d_ash_attention_sample_loss.yaml\", \"r\") as file_obj:\n",
    "    resnest26d_ash_attention_sample_loss = yaml.safe_load(file_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Because I need all the individual masks for my custom loss function, my Datasets \\_\\_getitem\\_\\_ function returns two labels if self.split is \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_mf_train = \"/home/albert/ml/Contrails/data/full_dataset/train/\"\n",
    "data_path_mf_valid = \"/home/albert/ml/Contrails/data/full_dataset/validation/\"\n",
    "\n",
    "class ContrailsDatasetMixed(Dataset):\n",
    "    def __init__(self, split=\"train\", mode=\"single\", delta_t = 0):\n",
    "        self.delta_t = delta_t\n",
    "        self.split = split\n",
    "        self.mode = mode\n",
    "        self.path = (lambda x: data_path_mf_train if x == \"train\" else data_path_mf_valid)(self.split)\n",
    "        self.examples = os.listdir(self.path)\n",
    "        self.normalize_image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    \n",
    "    def read_record(self, directory):\n",
    "        record_data = {}\n",
    "        for x in [\n",
    "            \"band_11\", \n",
    "            \"band_14\", \n",
    "            \"band_15\"\n",
    "        ]:\n",
    "            record_data[x] = np.load(os.path.join(directory, x + \".npy\"))\n",
    "\n",
    "        return record_data\n",
    "\n",
    "    def normalize_range(self, data, bounds):\n",
    "        \"\"\"Maps data to the range [0, 1].\"\"\"\n",
    "        return (data - bounds[0]) / (bounds[1] - bounds[0])\n",
    "    \n",
    "    def get_false_color(self, record_data):\n",
    "        _T11_BOUNDS = (243, 303)\n",
    "        _CLOUD_TOP_TDIFF_BOUNDS = (-4, 5)\n",
    "        _TDIFF_BOUNDS = (-4, 2)\n",
    "        r = self.normalize_range(record_data[\"band_15\"] - record_data[\"band_14\"], _TDIFF_BOUNDS)\n",
    "        g = self.normalize_range(record_data[\"band_14\"] - record_data[\"band_11\"], _CLOUD_TOP_TDIFF_BOUNDS)\n",
    "        b = self.normalize_range(record_data[\"band_14\"], _T11_BOUNDS)\n",
    "        false_color = np.clip(np.stack([r, g, b], axis=2), 0, 1)\n",
    "        if self.mode == \"single\":\n",
    "            t_null = 4\n",
    "            return false_color[..., t_null+self.delta_t]\n",
    "        else:\n",
    "            return false_color\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path = f\"{self.path}{self.examples[index]}\"\n",
    "        data = self.read_record(path)    \n",
    "        img = self.get_false_color(data)\n",
    "        if self.split == \"validation\":\n",
    "            label = np.load(os.path.join(path, \"human_pixel_masks.npy\")).squeeze()\n",
    "            label = torch.Tensor(label).to(torch.int64)\n",
    "        if self.split == \"train\":\n",
    "            label = np.load(os.path.join(path, \"human_pixel_masks.npy\")).squeeze()\n",
    "            label = torch.Tensor(label).to(torch.int64)\n",
    "            label_indiv = np.load(os.path.join(path, \"human_individual_masks.npy\")).squeeze()\n",
    "            labelers = label_indiv[1,1,:].shape[0]\n",
    "            label_union = torch.zeros([256, 256])\n",
    "            for i in range(labelers):\n",
    "                label_i = label_indiv[:, :,i]\n",
    "                label_union[label_i[:, :] == 1] = 1   \n",
    "\n",
    "        if self.mode == \"single\":\n",
    "            img = torch.tensor(np.reshape(img, (256, 256, 3, -1))).to(torch.float32).permute(3, 2, 0, 1).squeeze()\n",
    "        else:\n",
    "            img = torch.tensor(np.reshape(img, (256, 256, 3, -1))).to(torch.float32).permute(3, 2, 0, 1)\n",
    "\n",
    "        img = self.normalize_image(img)\n",
    "\n",
    "        if self.split in [\"train\"]:\n",
    "            return img.float(), [label.float(),label_union.float()]\n",
    "        if self.split in [\"validation\"]:\n",
    "            return img.float(), label.float()\n",
    "\n",
    "        return img.float()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightning Module Custom Tversky Loss\n",
    "This Lightning module implements the custom loss function.  \n",
    "The hyperparameters for the loss function are set in the config dictionary, but changed via the training function parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningModuleTrLoss(L.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # print(config[\"aux_params\"])\n",
    "        self.model = model = smp.Unet(\n",
    "            encoder_name=config[\"encoder_name\"],\n",
    "            encoder_depth=config[\"encoder_depth\"],\n",
    "            decoder_channels = (256, 128, 64, 32,16)[:config[\"encoder_depth\"]],\n",
    "            # aux_params = config[\"aux_params\"],\n",
    "            decoder_attention_type =config[\"decoder_attention_type\"],\n",
    "            encoder_weights=\"imagenet\",\n",
    "            in_channels=3,\n",
    "            classes=1,\n",
    "            activation=None,\n",
    "        )\n",
    "        self.loss_module = smp.losses.DiceLoss(mode=\"binary\", smooth=config[\"loss_smooth\"])\n",
    "        self.val_step_outputs = []\n",
    "        self.val_step_labels = []\n",
    "        self.alpha = config[\"alpha\"]\n",
    "        self.beta = config[\"beta\"]\n",
    "        self.alpha_union = config[\"alpha_union\"]\n",
    "        self.beta_union = config[\"beta_union\"]\n",
    "\n",
    "    def forward(self, batch):\n",
    "        imgs = batch\n",
    "        preds = self.model(imgs)\n",
    "        return preds\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), **self.config[\"optimizer_params\"])\n",
    "\n",
    "        if self.config[\"scheduler\"][\"name\"] == \"CosineAnnealingLR\":\n",
    "            scheduler = CosineAnnealingLR(\n",
    "                optimizer,\n",
    "                **self.config[\"scheduler\"][\"params\"][\"CosineAnnealingLR\"],\n",
    "            )\n",
    "            lr_scheduler_dict = {\"scheduler\": scheduler, \"interval\": \"step\"}\n",
    "            return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_dict}\n",
    "        elif self.config[\"scheduler\"][\"name\"] == \"ReduceLROnPlateau\":\n",
    "            scheduler = ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                **self.config[\"scheduler\"][\"params\"][\"ReduceLROnPlateau\"],\n",
    "            )\n",
    "            lr_scheduler = {\"scheduler\": scheduler, \"monitor\": \"val_loss\"}\n",
    "            return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler}\n",
    "        elif self.config[\"scheduler\"][\"name\"] == \"cosine_with_hard_restarts_schedule_with_warmup\":\n",
    "            scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n",
    "                optimizer,\n",
    "                **self.config[\"scheduler\"][\"params\"][self.config[\"scheduler\"][\"name\"]],\n",
    "            )\n",
    "            lr_scheduler_dict = {\"scheduler\": scheduler, \"interval\": \"step\"}\n",
    "            return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_dict}\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, [label_gt,label_union] = batch\n",
    "        preds = self.model(imgs)\n",
    "        if self.config[\"image_size\"] != 256:\n",
    "            preds = torch.nn.functional.interpolate(preds, size=256, mode='bilinear')\n",
    "        loss_ground_truth = smp.losses.TverskyLoss(\"binary\", classes=None, log_loss=False, from_logits=True, smooth=0.0, ignore_index=None, eps=1e-06, alpha=self.alpha, beta=self.beta, gamma=1.0)(preds, label_gt)\n",
    "        loss_union = smp.losses.TverskyLoss(\"binary\", classes=None, log_loss=False, from_logits=True, smooth=0.0, ignore_index=None, eps=1e-06, alpha=self.alpha_union, beta=self.beta_union, gamma=1.0)(preds, label_union)\n",
    "        loss = torch.mul(loss_ground_truth,self.config[\"weight_ground_truth\"])+torch.mul(loss_union, self.config[\"weight_union\"])\n",
    "        self.log('loss_union', float(loss_union) , on_step=False, on_epoch=True, prog_bar=True) #, batch_size=16\n",
    "        self.log('loss_ground_truth', float(loss_ground_truth) , on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        for param_group in self.trainer.optimizers[0].param_groups:\n",
    "            lr = param_group[\"lr\"]\n",
    "        self.log(\"lr\", lr, on_step=True, on_epoch=False, prog_bar=True)\n",
    "\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        preds = self.model(imgs)\n",
    "        if self.config[\"image_size\"] != 256:\n",
    "            preds = torch.nn.functional.interpolate(preds, size=256, mode='bilinear')\n",
    "        loss = self.loss_module(preds, labels)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.val_step_outputs.append(preds)\n",
    "        self.val_step_labels.append(labels)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        all_preds = torch.cat(self.val_step_outputs)\n",
    "        all_labels = torch.cat(self.val_step_labels)\n",
    "        all_preds = torch.sigmoid(all_preds)\n",
    "        self.val_step_outputs.clear()\n",
    "        self.val_step_labels.clear()\n",
    "        val_dice = dice(all_preds, all_labels.long())\n",
    "        self.log(\"val_dice\", val_dice, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        if self.trainer.global_rank == 0:\n",
    "            print(f\"\\nEpoch: {self.current_epoch}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to train the model with certain hyperparamters\n",
    "\n",
    "The sample_union_loss function can be used to train the model with different hyperparameters. The results are tracked via the csv_logger and can later be plotted to compare the performance of the model on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_union_loss(delta, cfg, module,alpha, alpha_union, weight_ground_truth):\n",
    "    cfg['model'][\"alpha\"] = alpha\n",
    "    cfg['model'][\"beta\"] = 1-alpha\n",
    "    cfg['model'][\"alpha_union\"] = alpha_union\n",
    "    cfg['model'][\"beta_union\"] = 1-alpha_union\n",
    "    cfg['model'][\"weight_ground_truth\"] = weight_ground_truth\n",
    "    cfg['model'][\"weight_union\"] = 1-weight_ground_truth\n",
    "    CHECK WHICH RESNEST I USED AND ADJUST BELOW\n",
    "    identifier = \"resnest101e_alpha=\"+str(alpha)+\"_alpha_union=\"+str(alpha_union)+\"_weight_ground_truth=\"+str(weight_ground_truth)\n",
    "    dataset_train = ContrailsDatasetMixed(\"train\", \"single\",delta)\n",
    "    dataset_validation = ContrailsDatasetMixed(\"validation\",\"single\", delta)\n",
    "    \n",
    "    data_loader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=cfg[\"train_bs\"],\n",
    "    shuffle=True,\n",
    "    num_workers=cfg[\"workers\"],\n",
    "    )\n",
    "    \n",
    "    data_loader_validation = DataLoader(\n",
    "        dataset_validation,\n",
    "        batch_size=cfg[\"valid_bs\"],\n",
    "        shuffle=False,\n",
    "        num_workers=cfg[\"workers\"],\n",
    "    )\n",
    "    \n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_dice\",\n",
    "    dirpath=cfg[\"output_dir\"],\n",
    "    mode=\"max\",\n",
    "    filename= identifier,\n",
    "    save_top_k=1,\n",
    "    verbose=1,\n",
    "    )\n",
    "    \n",
    "    progress_bar_callback = TQDMProgressBar(\n",
    "    refresh_rate=cfg[\"progress_bar_refresh_rate\"]\n",
    "    )\n",
    "\n",
    "    early_stop_callback = EarlyStopping(**cfg[\"early_stop\"])\n",
    "    csv_logger = CSVLogger(cfg[\"output_dir\"], name=identifier)\n",
    "    trainer = L.Trainer(\n",
    "        callbacks=[checkpoint_callback, early_stop_callback, progress_bar_callback], logger = csv_logger,\n",
    "        **cfg[\"trainer\"],\n",
    "    )\n",
    "    \n",
    "    # cfg[\"model\"][\"scheduler\"][\"params\"][\"CosineAnnealingLR\"][\"T_max\"] *= len(data_loader_train)/cfg[\"trainer\"][\"devices\"]\n",
    "    model = module(cfg[\"model\"])\n",
    "    \n",
    "    trainer.fit(model, data_loader_train, data_loader_validation)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: Dice/F1-Score of Ground truth only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "sample_union_loss(0, resnest26d_ash_attention_sample_loss, LightningModuleTrLoss, alpha = 0.5,alpha_union = 0.5, weight_ground_truth = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample tversky parameter and weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for weight_ground_truth in [0.6,0.7]:\n",
    "    for alpha in [0.35,0.5,0.65]:\n",
    "        for alpha_union in [0.4,0.6,0.8]:\n",
    "            # sample_union_loss(0, resnest26d_ash_attention_sample_loss, LightningModuleTrLoss, alpha = alpha,alpha_union = alpha_union, weight_ground_truth = weight_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TODO\n",
    "# Organize the metrics/ copy them into this folder such that they can be uploaded to github\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15,20))\n",
    "for weight_ground_truth in [0.6]:\n",
    "    print(\"\")\n",
    "    for alpha in [0.35,0.5,0.65]:\n",
    "        for alpha_union in [0.4,0.6,0.8]:\n",
    "            path = f\"/home/albert/ml/Contrails/notebooks/test_use_all_bands/resnest26d_ash_attention_sample_loss/resnest101e_alpha={alpha}_alpha_union={alpha_union}_weight_ground_truth={weight_ground_truth}/version_0/metrics.csv\"\n",
    "            label = f\"alpha={alpha}_alpha_union={alpha_union}_weight_ground_truth={weight_ground_truth}\"\n",
    "            metrics = pd.read_csv(path)\n",
    "            metrics = metrics.dropna(subset = ['val_dice']).set_index(\"epoch\")\n",
    "            linestyle = {0.4:\"dotted\",0.6:\"--\",0.8:\"-\"}[alpha_union]\n",
    "            color = {0.35:(0.6,0,0),0.5:(0,0,0.5),0.65:(0, 0.6, 0)}[alpha]\n",
    "            plt.plot(metrics['val_dice'], label=label, linestyle = linestyle, color = color)\n",
    "            maximum = metrics.dropna(subset = ['val_dice']).max()[\"val_dice\"]#[[\"val_dice\",\"epoch\"]]\n",
    "            print(label + \" max: {:.3f}\".format(maximum))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15,20))\n",
    "for weight_ground_truth in [0.7]:\n",
    "    print(\"\")\n",
    "    for alpha in [0.35,0.5,0.65]:\n",
    "        for alpha_union in [0.4,0.6,0.8]:\n",
    "            path = f\"/home/albert/ml/Contrails/notebooks/test_use_all_bands/resnest26d_ash_attention_sample_loss/resnest101e_alpha={alpha}_alpha_union={alpha_union}_weight_ground_truth={weight_ground_truth}/version_0/metrics.csv\"\n",
    "            label = f\"alpha={alpha}_alpha_union={alpha_union}_weight_ground_truth={weight_ground_truth}\"\n",
    "            metrics = pd.read_csv(path)\n",
    "            metrics = metrics.dropna(subset = ['val_dice']).set_index(\"epoch\")\n",
    "            linestyle = {0.4:\"dotted\",0.6:\"--\",0.8:\"-\"}[alpha_union]\n",
    "            color = {0.35:(0.6,0,0),0.5:(0,0,0.5),0.65:(0, 0.6, 0)}[alpha]\n",
    "            plt.plot(metrics['val_dice'], label=label, linestyle = linestyle, color = color)\n",
    "            maximum = metrics.dropna(subset = ['val_dice']).max()[\"val_dice\"]#[[\"val_dice\",\"epoch\"]]\n",
    "            print(label + \" max: {:.3f}\".format(maximum))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it seems like the model 0.5 0.6 0.7 seems to continue to improve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: \n",
    "Add below graphs and then check what I continued to explore, and reason for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.interpolate import griddata\n",
    "import numpy as np\n",
    "\n",
    "data = []\n",
    "dice_list = []\n",
    "for weight_ground_truth in [0.7]:\n",
    "    for alpha in [0.35,0.5,0.65]:\n",
    "        for alpha_union in [0.4,0.6,0.8]:\n",
    "            path = f\"/home/albert/ml/Contrails/notebooks/test_use_all_bands/resnest26d_ash_attention_sample_loss/resnest101e_alpha={alpha}_alpha_union={alpha_union}_weight_ground_truth={weight_ground_truth}/version_0/metrics.csv\"\n",
    "            label = f\"alpha={alpha}_alpha_union={alpha_union}_weight_ground_truth={weight_ground_truth}\"\n",
    "            metrics = pd.read_csv(path)\n",
    "            metrics = metrics.dropna(subset = ['val_dice']).set_index(\"epoch\")\n",
    "            maximum = metrics.dropna(subset = ['val_dice']).max()[\"val_dice\"]#[[\"val_dice\",\"epoch\"]]\n",
    "            data.append([alpha, alpha_union, maximum])\n",
    "            dice_list.append(f\" max: {maximum:.3f} \"+label)\n",
    "\n",
    "dice_list.sort\n",
    "for element in dice_list: print(element)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [i[0] for i in data]\n",
    "y = [i[1] for i in data]\n",
    "z = [i[2] for i in data]\n",
    "\n",
    "# plt.figure(figsize=(15,15))\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Create a mesh grid\n",
    "X,Y = np.meshgrid(x,y)\n",
    "\n",
    "# Interpolate the data\n",
    "from scipy.interpolate import griddata\n",
    "Z = griddata((x,y), z, (X,Y), method='cubic')\n",
    "\n",
    "# Plot the points with a color gradient\n",
    "ax.scatter(x,y,z,c=z,cmap='coolwarm', alpha=1)\n",
    "\n",
    "# Add a color bar which maps values to colors\n",
    "# fig.colorbar(surf)\n",
    "\n",
    "# Plot lines from points to surface\n",
    "for i in range(len(x)):\n",
    "    ax.plot([x[i], x[i]], [y[i], y[i]], [0.615, z[i]], color='black', alpha=0.5)\n",
    "\n",
    "# Plot the surface\n",
    "ax.set_xlabel('alpha')\n",
    "ax.set_ylabel('alpha_union')\n",
    "ax.set_zlabel('maximum')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fst Conclusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Check to see which config I used in my results!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for weight_ground_truth in [0.6,0.7,0.8]:\n",
    "    for alpha in [0.35,0.5,0.65]:\n",
    "        for alpha_union in [0.4,0.6,0.8]:\n",
    "            sample_union_loss(0, resnest26d_ash_attention_sample_loss, LightningModuleTrLoss, alpha = alpha,alpha_union = alpha_union, weight_ground_truth = weight_ground_truth)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot individual masks of specific example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
